{"id":"paper_1702500000001","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","authors":"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","journal":"NAACL","year":"2019","doi":"10.18653/v1/N19-1423","url":"https://arxiv.org/abs/1810.04805","keyAssumptions":"Unidirectional language models are sufficient for pre-training; Fine-tuning requires task-specific architectures","citation":"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT (pp. 4171-4186).","notes":"Introduces bidirectional pre-training using masked language modeling. Shows that deep bidirectional representations significantly improve downstream task performance.","addedDate":"2024-01-15T10:05:00Z"}
{"id":"paper_rp_lit_001","title":"MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders","authors":"Xuezhe Ma, Chunting Zhou, Eduard Hovy","journal":"ICLR","year":"2019","doi":"","url":"https://arxiv.org/pdf/1901.01498.pdf","keyAssumptions":"ELBO optimization naturally leads to meaningful latent representations; KL divergence sufficient for posterior regularization","citation":"Ma, X., Zhou, C., & Hovy, E. (2019). MAE: Mutual posterior-divergence regularization for variational autoencoders. ICLR 2019.","notes":"Introduces mutual posterior-divergence regularization controlling latent space geometry. Addresses posterior collapse by encouraging diversity between posterior distributions of different samples.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_002","title":"Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness","authors":"Dazhong Shen, Chuan Qin, Chao Wang, Hengshu Zhu, Enhong Chen, Hui Xiong","journal":"","year":"2021","doi":"","url":"https://arxiv.org/pdf/2110.12381v1.pdf","keyAssumptions":"Standard VAE training with ELBO sufficient for meaningful representation learning; KL regularization alone prevents posterior collapse","citation":"Shen, D., Qin, C., Wang, C., Zhu, H., Chen, E., & Xiong, H. (2021). Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness. arXiv preprint arXiv:2110.12381.","notes":"DU-VAE uses Dropout on variances and Batch-Normalization on means to implicitly regularize posterior distributions for high diversity and low uncertainty.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_003","title":"CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse","authors":"Fotios Lygerakis, Elmar Rueckert","journal":"","year":"2023","doi":"","url":"https://arxiv.org/pdf/2309.02968.pdf","keyAssumptions":"ELBO objective alone maintains input-latent dependence; KL regularization sufficient to prevent posterior collapse","citation":"Lygerakis, F., & Rueckert, E. (2023). CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. arXiv preprint arXiv:2309.02968.","notes":"Augments VAE with contrastive objective maximizing mutual information between similar visual inputs' representations to prevent posterior collapse.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_004","title":"Wasserstein Auto-Encoders","authors":"Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf","journal":"","year":"2017","doi":"","url":"https://arxiv.org/abs/1711.01558","keyAssumptions":"KL divergence optimal for autoencoder regularization; Variational inference framework necessary for principled training","citation":"Tolstikhin, I., Bousquet, O., Gelly, S., & Schoelkopf, B. (2017). Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558.","notes":"Replaces KL regularization with Wasserstein distance minimization, providing more flexible regularization and better sample quality than VAEs.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_005","title":"Adversarial Autoencoders","authors":"Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey","journal":"","year":"2015","doi":"","url":"https://arxiv.org/abs/1511.05644v1","keyAssumptions":"Autoencoder latent spaces naturally follow meaningful distributions; Prior distributions must be analytically tractable","citation":"Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B. (2015). Adversarial autoencoders. arXiv preprint arXiv:1511.05644.","notes":"Uses adversarial training to match aggregated posterior to arbitrary prior distribution, enabling arbitrary prior imposition beyond Gaussian.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_006","title":"Bridging the ELBO and MMD","authors":"Talip Ucar","journal":"","year":"2019","doi":"","url":"https://arxiv.org/pdf/1910.13181v1.pdf","keyAssumptions":"ELBO with KL divergence optimal for variational autoencoders; KL provides adequate posterior regularization","citation":"Ucar, T. (2019). Bridging the ELBO and MMD. arXiv preprint arXiv:1910.13181.","notes":"Î¼-VAE replaces KL term with MMD objective and introduces latent clipping. Shows MMD more robust against posterior collapse than KL divergence.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_007","title":"Cauchy-Schwarz Regularized Autoencoder","authors":"Linh Tran, Maja Pantic, Marc Peter Deisenroth","journal":"","year":"2021","doi":"","url":"https://arxiv.org/pdf/2101.02149v1.pdf","keyAssumptions":"Gaussian priors sufficient for VAE applications; KL divergence necessary for tractable optimization","citation":"Tran, L., Pantic, M., & Deisenroth, M. P. (2021). Cauchy-Schwarz regularized autoencoder. arXiv preprint arXiv:2101.02149.","notes":"Introduces Cauchy-Schwarz divergence enabling analytical computation for GMM priors, allowing richer multi-modal priors in autoencoding framework.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_008","title":"Controlling Posterior Collapse by an Inverse Lipschitz Neural Network","authors":"Yuri Kinoshita, Kenta Oono, Kenji Fukumizu, Yuichi Yoshida, Shin-ichi Maeda","journal":"ICML","year":"2024","doi":"","url":"https://arxiv.org/html/2304.12770v2","keyAssumptions":"Posterior collapse controllable through loss modifications; Architecture-agnostic solutions not possible","citation":"Kinoshita, Y., Oono, K., Fukumizu, K., Yoshida, Y., & Maeda, S. I. (2024). Controlling posterior collapse by an inverse Lipschitz neural network. ICML 2024.","notes":"First method providing theoretical guarantees for posterior collapse control through inverse Lipschitz constraints in decoder architecture.","addedDate":"2025-08-05T05:01:00Z"}