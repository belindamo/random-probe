{"id":"paper_rp_lit_001","title":"MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders","authors":"Xuezhe Ma, Chunting Zhou, Eduard Hovy","journal":"ICLR","year":"2019","doi":"","url":"https://arxiv.org/pdf/1901.01498.pdf","keyAssumptions":"ELBO optimization naturally leads to meaningful latent representations; KL divergence sufficient for posterior regularization","citation":"Ma, X., Zhou, C., & Hovy, E. (2019). MAE: Mutual posterior-divergence regularization for variational autoencoders. ICLR 2019.","notes":"Introduces mutual posterior-divergence regularization controlling latent space geometry. Addresses posterior collapse by encouraging diversity between posterior distributions of different samples.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_002","title":"Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness","authors":"Dazhong Shen, Chuan Qin, Chao Wang, Hengshu Zhu, Enhong Chen, Hui Xiong","journal":"","year":"2021","doi":"","url":"https://arxiv.org/pdf/2110.12381v1.pdf","keyAssumptions":"Standard VAE training with ELBO sufficient for meaningful representation learning; KL regularization alone prevents posterior collapse","citation":"Shen, D., Qin, C., Wang, C., Zhu, H., Chen, E., & Xiong, H. (2021). Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness. arXiv preprint arXiv:2110.12381.","notes":"DU-VAE uses Dropout on variances and Batch-Normalization on means to implicitly regularize posterior distributions for high diversity and low uncertainty.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_003","title":"CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse","authors":"Fotios Lygerakis, Elmar Rueckert","journal":"","year":"2023","doi":"","url":"https://arxiv.org/pdf/2309.02968.pdf","keyAssumptions":"ELBO objective alone maintains input-latent dependence; KL regularization sufficient to prevent posterior collapse","citation":"Lygerakis, F., & Rueckert, E. (2023). CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. arXiv preprint arXiv:2309.02968.","notes":"Augments VAE with contrastive objective maximizing mutual information between similar visual inputs' representations to prevent posterior collapse.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_004","title":"Wasserstein Auto-Encoders","authors":"Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf","journal":"","year":"2017","doi":"","url":"https://arxiv.org/abs/1711.01558","keyAssumptions":"KL divergence optimal for autoencoder regularization; Variational inference framework necessary for principled training","citation":"Tolstikhin, I., Bousquet, O., Gelly, S., & Schoelkopf, B. (2017). Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558.","notes":"Replaces KL regularization with Wasserstein distance minimization, providing more flexible regularization and better sample quality than VAEs.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_005","title":"Adversarial Autoencoders","authors":"Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey","journal":"","year":"2015","doi":"","url":"https://arxiv.org/abs/1511.05644v1","keyAssumptions":"Autoencoder latent spaces naturally follow meaningful distributions; Prior distributions must be analytically tractable","citation":"Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B. (2015). Adversarial autoencoders. arXiv preprint arXiv:1511.05644.","notes":"Uses adversarial training to match aggregated posterior to arbitrary prior distribution, enabling arbitrary prior imposition beyond Gaussian.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_006","title":"Bridging the ELBO and MMD","authors":"Talip Ucar","journal":"","year":"2019","doi":"","url":"https://arxiv.org/pdf/1910.13181v1.pdf","keyAssumptions":"ELBO with KL divergence optimal for variational autoencoders; KL provides adequate posterior regularization","citation":"Ucar, T. (2019). Bridging the ELBO and MMD. arXiv preprint arXiv:1910.13181.","notes":"μ-VAE replaces KL term with MMD objective and introduces latent clipping. Shows MMD more robust against posterior collapse than KL divergence.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_007","title":"Cauchy-Schwarz Regularized Autoencoder","authors":"Linh Tran, Maja Pantic, Marc Peter Deisenroth","journal":"","year":"2021","doi":"","url":"https://arxiv.org/pdf/2101.02149v1.pdf","keyAssumptions":"Gaussian priors sufficient for VAE applications; KL divergence necessary for tractable optimization","citation":"Tran, L., Pantic, M., & Deisenroth, M. P. (2021). Cauchy-Schwarz regularized autoencoder. arXiv preprint arXiv:2101.02149.","notes":"Introduces Cauchy-Schwarz divergence enabling analytical computation for GMM priors, allowing richer multi-modal priors in autoencoding framework.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_008","title":"Controlling Posterior Collapse by an Inverse Lipschitz Neural Network","authors":"Yuri Kinoshita, Kenta Oono, Kenji Fukumizu, Yuichi Yoshida, Shin-ichi Maeda","journal":"ICML","year":"2024","doi":"","url":"https://arxiv.org/html/2304.12770v2","keyAssumptions":"Posterior collapse controllable through loss modifications; Architecture-agnostic solutions not possible","citation":"Kinoshita, Y., Oono, K., Fukumizu, K., Yoshida, Y., & Maeda, S. I. (2024). Controlling posterior collapse by an inverse Lipschitz neural network. ICML 2024.","notes":"First method providing theoretical guarantees for posterior collapse control through inverse Lipschitz constraints in decoder architecture.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_009","title":"ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders","authors":"Fotios Lygerakis, Elmar Rueckert","journal":"","year":"2024","doi":"","url":"https://arxiv.org/abs/2407.06797","keyAssumptions":"Standard ELBO formulation provides sufficient interpretability and control for VAE optimization","citation":"Lygerakis, F., & Rueckert, E. (2024). ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders. arXiv preprint arXiv:2407.06797.","notes":"Reformulates ELBO into mutual information, entropy, and cross-entropy components, enabling explicit control over information flow and integration of complex priors. Provides theoretical foundation for Random Probe's explicit distributional control approach.","addedDate":"2025-08-08T18:59:00Z"}
{"id":"paper_rp_lit_010","title":"A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse","authors":"Zhongliang Guo, Chun Tong Lei, Lei Fang, Shuai Zhao, Yifei Qian, Jingyu Lin, Zeyu Wang, Cunjian Chen, Ognjen Arandjelović, Chun Pong Lau","journal":"","year":"2024","doi":"","url":"https://arxiv.org/abs/2408.10901","keyAssumptions":"VAE distributional problems are primarily generation quality concerns, not exploitable security vulnerabilities","citation":"Guo, Z., Lei, C. T., Fang, L., et al. (2024). A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse. arXiv preprint arXiv:2408.10901.","notes":"Demonstrates that VAE posterior collapse can be exploited as security vulnerability with minimal model access. Provides direct empirical validation that distributional conformity issues are fundamental problems requiring direct enforcement methods like Random Probe.","addedDate":"2025-08-08T18:59:00Z"}
{"id":"paper_rp_lit_011","title":"S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling","authors":"Suman Adhya, et al.","journal":"","year":"2025","doi":"","url":"https://arxiv.org/abs/2507.12451","keyAssumptions":"von Mises-Fisher priors with KL divergence sufficient for hyperspherical VAE training","citation":"Adhya, S., et al. (2025). S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling. arXiv preprint arXiv:2507.12451.","notes":"Uses Spherical Sliced-Wasserstein distance on unit hypersphere to address posterior collapse in topic modeling. Validates Random Probe's insight that alternative distance measures outperform KL divergence for distributional matching.","addedDate":"2025-08-08T18:59:00Z"}
{"id":"paper_rp_lit_012","title":"Probabilistic Variational Contrastive Learning","authors":"Minoh Jeong, Seonho Kim, Alfred Hero","journal":"","year":"2025","doi":"","url":"https://arxiv.org/abs/2506.10159","keyAssumptions":"Deterministic embeddings adequate for contrastive representation learning","citation":"Jeong, M., Kim, S., & Hero, A. (2025). Probabilistic Variational Contrastive Learning. arXiv preprint arXiv:2506.10159.","notes":"Interprets contrastive learning through VAE framework with uniform prior on unit hypersphere. Demonstrates that distributional thinking improves representation learning and mitigates dimensional collapse.","addedDate":"2025-08-08T18:59:00Z"}
{"id":"paper_rp_lit_013","title":"Preventing Model Collapse in Gaussian Process Latent Variable Models","authors":"Ying Li, Zhidi Lin, Feng Yin, Michael Minyi Zhang","journal":"ICML","year":"2024","doi":"","url":"https://arxiv.org/abs/2404.01697","keyAssumptions":"Standard GP kernels and fixed projection noise sufficient for meaningful latent representations","citation":"Li, Y., Lin, Z., Yin, F., & Zhang, M. M. (2024). Preventing Model Collapse in Gaussian Process Latent Variable Models. ICML 2024.","notes":"Uses spectral mixture kernels with random Fourier features to prevent model collapse. Shows that distributional conformity issues affect broader class of latent variable models beyond VAEs, supporting Random Probe's general approach.","addedDate":"2025-08-08T18:59:00Z"}
{"id":"paper_rp_lit_014","title":"Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse","authors":"Hien Dang, Tho Tran Huu, Tan Minh Nguyen, Nhat Ho","journal":"ICLR","year":"2024","doi":"","url":"https://openreview.net/forum?id=4zZFGliCl9","keyAssumptions":"Posterior collapse mechanisms are similar across different VAE variants","citation":"Dang, H., Huu, T. T., Nguyen, T. M., & Ho, N. (2024). Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse occurrence for conditional and hierarchical variational autoencoders. ICLR 2024.","notes":"Provides theoretical analysis of posterior collapse in conditional and hierarchical VAEs. Expands understanding of distributional conformity issues beyond standard VAEs, supporting Random Probe's broad applicability.","addedDate":"2025-08-08T18:59:00Z"}
{"id":"paper_rp_lit_015","title":"α-TCVAE: On the relationship between Disentanglement and Diversity","authors":"Cristian Meo, Louis Mahon, Anirudh Goyal, Justin Dauwels","journal":"","year":"2024","doi":"","url":"https://arxiv.org/abs/2411.00588","keyAssumptions":"Standard β-VAE provides optimal trade-off between disentanglement and reconstruction quality","citation":"Meo, C., Mahon, L., Goyal, A., & Dauwels, J. (2024). α-TCVAE: On the relationship between Disentanglement and Diversity. arXiv preprint arXiv:2411.00588.","notes":"Novel total correlation bound combining VIB and CEB terms for improved disentanglement and diversity. Demonstrates connection between disentanglement and generative diversity, supporting Random Probe's focus on distributional conformity.","addedDate":"2025-08-08T18:59:00Z"}