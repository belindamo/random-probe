{"id":"paper_rp_lit_001","title":"MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders","authors":"Xuezhe Ma, Chunting Zhou, Eduard Hovy","journal":"ICLR","year":"2019","doi":"","url":"https://arxiv.org/pdf/1901.01498.pdf","keyAssumptions":"ELBO optimization naturally leads to meaningful latent representations; KL divergence sufficient for posterior regularization","citation":"Ma, X., Zhou, C., & Hovy, E. (2019). MAE: Mutual posterior-divergence regularization for variational autoencoders. ICLR 2019.","notes":"Introduces mutual posterior-divergence regularization controlling latent space geometry. Addresses posterior collapse by encouraging diversity between posterior distributions of different samples.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_002","title":"Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness","authors":"Dazhong Shen, Chuan Qin, Chao Wang, Hengshu Zhu, Enhong Chen, Hui Xiong","journal":"","year":"2021","doi":"","url":"https://arxiv.org/pdf/2110.12381v1.pdf","keyAssumptions":"Standard VAE training with ELBO sufficient for meaningful representation learning; KL regularization alone prevents posterior collapse","citation":"Shen, D., Qin, C., Wang, C., Zhu, H., Chen, E., & Xiong, H. (2021). Regularizing Variational Autoencoder with Diversity and Uncertainty Awareness. arXiv preprint arXiv:2110.12381.","notes":"DU-VAE uses Dropout on variances and Batch-Normalization on means to implicitly regularize posterior distributions for high diversity and low uncertainty.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_003","title":"CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse","authors":"Fotios Lygerakis, Elmar Rueckert","journal":"","year":"2023","doi":"","url":"https://arxiv.org/pdf/2309.02968.pdf","keyAssumptions":"ELBO objective alone maintains input-latent dependence; KL regularization sufficient to prevent posterior collapse","citation":"Lygerakis, F., & Rueckert, E. (2023). CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. arXiv preprint arXiv:2309.02968.","notes":"Augments VAE with contrastive objective maximizing mutual information between similar visual inputs' representations to prevent posterior collapse.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_004","title":"Wasserstein Auto-Encoders","authors":"Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf","journal":"","year":"2017","doi":"","url":"https://arxiv.org/abs/1711.01558","keyAssumptions":"KL divergence optimal for autoencoder regularization; Variational inference framework necessary for principled training","citation":"Tolstikhin, I., Bousquet, O., Gelly, S., & Schoelkopf, B. (2017). Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558.","notes":"Replaces KL regularization with Wasserstein distance minimization, providing more flexible regularization and better sample quality than VAEs.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_005","title":"Adversarial Autoencoders","authors":"Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey","journal":"","year":"2015","doi":"","url":"https://arxiv.org/abs/1511.05644v1","keyAssumptions":"Autoencoder latent spaces naturally follow meaningful distributions; Prior distributions must be analytically tractable","citation":"Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B. (2015). Adversarial autoencoders. arXiv preprint arXiv:1511.05644.","notes":"Uses adversarial training to match aggregated posterior to arbitrary prior distribution, enabling arbitrary prior imposition beyond Gaussian.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_006","title":"Bridging the ELBO and MMD","authors":"Talip Ucar","journal":"","year":"2019","doi":"","url":"https://arxiv.org/pdf/1910.13181v1.pdf","keyAssumptions":"ELBO with KL divergence optimal for variational autoencoders; KL provides adequate posterior regularization","citation":"Ucar, T. (2019). Bridging the ELBO and MMD. arXiv preprint arXiv:1910.13181.","notes":"Î¼-VAE replaces KL term with MMD objective and introduces latent clipping. Shows MMD more robust against posterior collapse than KL divergence.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_007","title":"Cauchy-Schwarz Regularized Autoencoder","authors":"Linh Tran, Maja Pantic, Marc Peter Deisenroth","journal":"","year":"2021","doi":"","url":"https://arxiv.org/pdf/2101.02149v1.pdf","keyAssumptions":"Gaussian priors sufficient for VAE applications; KL divergence necessary for tractable optimization","citation":"Tran, L., Pantic, M., & Deisenroth, M. P. (2021). Cauchy-Schwarz regularized autoencoder. arXiv preprint arXiv:2101.02149.","notes":"Introduces Cauchy-Schwarz divergence enabling analytical computation for GMM priors, allowing richer multi-modal priors in autoencoding framework.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_008","title":"Controlling Posterior Collapse by an Inverse Lipschitz Neural Network","authors":"Yuri Kinoshita, Kenta Oono, Kenji Fukumizu, Yuichi Yoshida, Shin-ichi Maeda","journal":"ICML","year":"2024","doi":"","url":"https://arxiv.org/html/2304.12770v2","keyAssumptions":"Posterior collapse controllable through loss modifications; Architecture-agnostic solutions not possible","citation":"Kinoshita, Y., Oono, K., Fukumizu, K., Yoshida, Y., & Maeda, S. I. (2024). Controlling posterior collapse by an inverse Lipschitz neural network. ICML 2024.","notes":"First method providing theoretical guarantees for posterior collapse control through inverse Lipschitz constraints in decoder architecture.","addedDate":"2025-08-05T05:01:00Z"}
{"id":"paper_rp_lit_009","title":"Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders","authors":"Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/pdf/2306.05023.pdf","keyAssumptions":"Standard VAE training sufficient for conditional and hierarchical VAEs; posterior collapse only relevant to standard VAEs","citation":"Dang, H., Tran, T., Nguyen, T., & Ho, N. (2024). Beyond vanilla variational autoencoders: Detecting posterior collapse in conditional and hierarchical variational autoencoders. arXiv preprint arXiv:2306.05023.","notes":"Advances theoretical understanding of posterior collapse to conditional and hierarchical VAEs. Proves correlation between input/output in conditional VAE and learnable encoder variance in hierarchical VAE cause collapse.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_010","title":"Uniform Transformation: Refining Latent Representation in Variational Autoencoders","authors":"Ye Shi, C. S. George Lee","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/html/2407.02681v1","keyAssumptions":"Regular latent distributions sufficient for VAE performance; traditional KL divergence handles latent space structure adequately","citation":"Shi, Y., & Lee, C. S. G. (2024). Uniform transformation: Refining latent representation in variational autoencoders. arXiv preprint arXiv:2407.02681.","notes":"Introduces three-stage Uniform Transformation module (G-KDE clustering, GM modeling, PIT) to address irregular latent distributions and posterior collapse through uniform latent space reconfiguration.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_011","title":"How to train your VAE","authors":"Mariano Rivera","journal":"arXiv","year":"2024","doi":"","url":"https://arxiv.org/abs/2309.13160","keyAssumptions":"Standard ELBO with single Gaussian posterior sufficient; individual variable distributions naturally structured by KL term","citation":"Rivera, M. (2024). How to train your VAE. arXiv preprint arXiv:2309.13160.","notes":"Redefines ELBO with mixture of Gaussians for posterior, introduces regularization to prevent variance collapse, uses PatchGAN discriminator for texture realism.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_012","title":"Preventing posterior collapse in variational autoencoders for text generation via decoder regularization","authors":"Alban Petit, Caio Corro","journal":"arXiv","year":"2021","doi":"","url":"https://arxiv.org/pdf/2110.14945v1.pdf","keyAssumptions":"Posterior collapse primarily encoder problem; decoder modifications insufficient for preventing collapse","citation":"Petit, A., & Corro, C. (2021). Preventing posterior collapse in variational autoencoders for text generation via decoder regularization. arXiv preprint arXiv:2110.14945.","notes":"Proposes fraternal dropout-based decoder regularization for text VAEs. Shows decoder-side modifications can effectively prevent posterior collapse in NLP applications.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_013","title":"Posterior Collapse and Latent Variable Non-identifiability","authors":"Yixin Wang, David M Blei, John P Cunningham","journal":"NeurIPS","year":"2023","doi":"","url":"https://arxiv.org/pdf/2301.00537.pdf","keyAssumptions":"Posterior collapse is neural network or optimization issue; identifiability not fundamental concern in VAE design","citation":"Wang, Y., Blei, D. M., & Cunningham, J. P. (2023). Posterior collapse and latent variable non-identifiability. NeurIPS 2023.","notes":"Proves posterior collapse occurs if and only if latent variables are non-identifiable. Proposes latent-identifiable VAEs using bijective Brenier maps and input convex neural networks.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_014","title":"Discouraging posterior collapse in hierarchical Variational Autoencoders using context","authors":"Anna Kuzina","journal":"arXiv","year":"2023","doi":"","url":"https://arxiv.org/abs/2302.09976","keyAssumptions":"Hierarchical VAEs naturally avoid posterior collapse; top-down architecture sufficient for deep latent learning","citation":"Kuzina, A. (2023). Discouraging posterior collapse in hierarchical variational autoencoders using context. arXiv preprint arXiv:2302.09976.","notes":"Shows hierarchical VAEs still suffer from posterior collapse. Proposes context-based modification using Discrete Cosine Transform for top latent variable to improve latent utilization.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_015","title":"Gromov-Wasserstein Autoencoders","authors":"Nao Nakagawa, Ren Togo, Takahiro Ogawa, Miki Haseyama","journal":"ICLR","year":"2023","doi":"","url":"https://arxiv.org/pdf/2209.07007.pdf","keyAssumptions":"Likelihood-based objectives necessary for VAE training; same-dimensional distributions required for meaningful comparison","citation":"Nakagawa, N., Togo, R., Ogawa, T., & Haseyama, M. (2023). Gromov-wasserstein autoencoders. ICLR 2023.","notes":"Replaces likelihood objectives with Gromov-Wasserstein metric between latent and data distributions. Handles different dimensionalities and provides structure-oriented discrepancy measure.","addedDate":"2025-08-06T06:21:00Z"}
{"id":"paper_rp_lit_016","title":"Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions","authors":"Songxuan Shi","journal":"arXiv","year":"2025","doi":"","url":"https://arxiv.org/abs/2507.17255","keyAssumptions":"Probabilistic assumptions necessary for VAE training; continuous latent representations superior to discrete","citation":"Shi, S. (2025). Rethinking VAE: From continuous to discrete representations without probabilistic assumptions. arXiv preprint arXiv:2507.17255.","notes":"Explores AE generative potential without probabilistic assumptions. Proposes clustering-based training connecting VAEs and VQ-VAEs through compactness and dispersion control.","addedDate":"2025-08-06T06:21:00Z"}